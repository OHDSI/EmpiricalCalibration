% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/EmpiricalCalibration.R
\name{calibrateP}
\alias{calibrateP}
\title{Calibrate the p-value}
\usage{
calibrateP(null, logRr, seLogRr, pValueConfidenceInterval = FALSE)
}
\arguments{
\item{null}{An object of class \code{null} created using the \code{fitNull}
function or an object of class \code{mcmcNull} created using the
\code{fitMcmcNull} function}

\item{logRr}{A numeric vector of one or more effect estimates on the log scale}

\item{seLogRr}{The standard error of the log of the effect estimates. Hint: often
the standard error = (log(<lower bound 95 percent confidence
interval>) - log(<effect estimate>))/qnorm(0.025)}

\item{pValueConfidenceInterval}{If true, computes the 95 percent confidence interval of the
                                  calibrated p-value}
}
\value{
A two-sided calibrated p-value.
}
\description{
\code{calibrateP} computes calibrated p-values using the fitted null distribution
}
\details{
This function computes a calibrated two-sided p-value as described in Schuemie et al (2014).
}
\examples{
data(sccs)
negatives <- sccs[sccs$groundTruth == 0, ]
null <- fitNull(negatives$logRr, negatives$seLogRr)
positive <- sccs[sccs$groundTruth == 1, ]
calibrateP(null, positive$logRr, positive$seLogRr)
}
\references{
Schuemie MJ, Ryan PB, Dumouchel W, Suchard MA, Madigan D. Interpreting observational studies: why
empirical calibration is needed to correct p-values. Statistics in Medicine 33(2):209-18,2014
}

