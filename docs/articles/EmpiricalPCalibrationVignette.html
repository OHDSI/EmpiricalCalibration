<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Empirical calibration of p-values • EmpiricalCalibration</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Empirical calibration of p-values">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">EmpiricalCalibration</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">3.1.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/EmpiricalCiCalibrationVignette.html">Empirical calibration of confidence intervals</a>
    </li>
    <li>
      <a href="../articles/EmpiricalMaxSprtCalibrationVignette.html">Empirical calibration and MaxSPRT</a>
    </li>
    <li>
      <a href="../articles/EmpiricalPCalibrationVignette.html">Empirical calibration of p-values</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://ohdsi.github.io/Hades" class="external-link"><img src='https://ohdsi.github.io/Hades/images/hadesMini.png' width=80 height=17 style='vertical-align: top;'></a>
</li>
<li>
  <a href="https://github.com/OHDSI/EmpiricalCalibration/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Empirical calibration of p-values</h1>
                        <h4 data-toc-skip class="author">Martijn J.
Schuemie, Marc A. Suchard</h4>
            
            <h4 data-toc-skip class="date">2024-09-30</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/OHDSI/EmpiricalCalibration/blob/HEAD/vignettes/EmpiricalPCalibrationVignette.Rmd" class="external-link"><code>vignettes/EmpiricalPCalibrationVignette.Rmd</code></a></small>
      <div class="hidden name"><code>EmpiricalPCalibrationVignette.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>In observational studies, there is always the possibility that an
effect size estimate is biased. This can be true even for advanced, well
thought out study designs, because of unmeasured or unmodeled
confounding. Negative controls (test-hypotheses where the exposure is
not believed to cause the outcome) can be used to detect the potential
for bias in a study, and with enough negative controls we can start to
estimate the systematic error distribution inherent in an observational
analysis. We can then use this estimated distribution to compute a
calibrated p-value, which reflects the probability of observing an
effect size estimate when the null hypothesis (of no effect) is true,
taking both systematic and random error into account.</p>
<p>In this document we will use an example study to illustrate how this
can be done using the <code>EmpiricalCalibration</code> R package. In
the example, we will try to answer the question whether sertraline (an
SSRI) causes GI bleeding. We use a Self-Controlled Case Series (SCCS)
design, and have applied this to a large insurance claims database.</p>
<p>The results from this study are available in the package, and can be
loaded using the <code><a href="https://rdrr.io/r/utils/data.html" class="external-link">data()</a></code> command:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">sccs</span><span class="op">)</span></span>
<span><span class="va">drugOfInterest</span> <span class="op">&lt;-</span> <span class="va">sccs</span><span class="op">[</span><span class="va">sccs</span><span class="op">$</span><span class="va">groundTruth</span> <span class="op">==</span> <span class="fl">1</span>, <span class="op">]</span></span>
<span><span class="va">drugOfInterest</span></span></code></pre></div>
<pre><code><span><span class="co">##     drugName groundTruth     logRr    seLogRr</span></span>
<span><span class="co">## 6 Sertraline           1 0.7326235 0.07371708</span></span></code></pre>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">drugOfInterest</span><span class="op">$</span><span class="va">logRr</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2.080532</span></span></code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/computeTraditionalP.html">computeTraditionalP</a></span><span class="op">(</span><span class="va">drugOfInterest</span><span class="op">$</span><span class="va">logRr</span>, <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">seLogRr</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 0</span></span></code></pre>
<p>Here we see that the effect estimate for sertraline is 2.1, with a
p-value that is so small R rounds it to 0.</p>
</div>
<div class="section level2">
<h2 id="negative-controls">Negative controls<a class="anchor" aria-label="anchor" href="#negative-controls"></a>
</h2>
<p>Negative controls are drug-outcome pairs where we believe the drug
does not cause (or prevent) the outcome. In other words, we believe the
true effect size to be a relative risk of 1. We would prefer our
negative controls to have some resemblance with out hypothesis of
interest (in our example sertraline - GI bleed), and we therefore
typically pick negative controls where the outcome is the same (exposure
controls), or the exposure is the same (outcome controls). In this
example, we have opted for exposure controls, and have identified a set
of drugs not believed to cause GI bleed. We have executed exactly the
same analysis for these exposures, resulting in a set of effect size
estimates, one per negative control:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">sccs</span><span class="op">)</span></span>
<span><span class="va">negatives</span> <span class="op">&lt;-</span> <span class="va">sccs</span><span class="op">[</span><span class="va">sccs</span><span class="op">$</span><span class="va">groundTruth</span> <span class="op">==</span> <span class="fl">0</span>, <span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##           drugName groundTruth     logRr   seLogRr</span></span>
<span><span class="co">## 1      Thiothixene           0 0.4339021 0.7617538</span></span>
<span><span class="co">## 2    Methocarbamol           0 0.6363184 0.1839892</span></span>
<span><span class="co">## 4      Phentermine           0 0.9297549 0.2979802</span></span>
<span><span class="co">## 5       Disulfiram           0 1.6919273 0.5955222</span></span>
<span><span class="co">## 7         Orlistat           0 0.5261691 0.1967199</span></span>
<span><span class="co">## 8 Prochlorperazine           0 0.8581890 0.1308460</span></span></code></pre>
<div class="section level3">
<h3 id="plot-negative-control-effect-sizes">Plot negative control effect sizes<a class="anchor" aria-label="anchor" href="#plot-negative-control-effect-sizes"></a>
</h3>
<p>We can start by creating a forest plot of our negative controls:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotForest.html">plotForest</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">$</span><span class="va">logRr</span>, <span class="va">negatives</span><span class="op">$</span><span class="va">seLogRr</span>, <span class="va">negatives</span><span class="op">$</span><span class="va">drugName</span><span class="op">)</span></span></code></pre></div>
<p><img src="EmpiricalPCalibrationVignette_files/figure-html/unnamed-chunk-4-1.png" width="700"></p>
<p>Here we see that many negative controls have a confidence interval
that does not include a relative risk of 1 (orange lines), certainly
more than the expected 5%. This indicates the analysis has systematic
error.</p>
</div>
</div>
<div class="section level2">
<h2 id="empirical-null-distribution">Empirical null distribution<a class="anchor" aria-label="anchor" href="#empirical-null-distribution"></a>
</h2>
<div class="section level3">
<h3 id="fitting-the-null-distribution">Fitting the null distribution<a class="anchor" aria-label="anchor" href="#fitting-the-null-distribution"></a>
</h3>
<p>We can use the negative controls to estimate the systematic error
distribution. We assume the distribution is a Gaussian distribution,
which we have found to give good performance in the past.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">null</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fitNull.html">fitNull</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">$</span><span class="va">logRr</span>, <span class="va">negatives</span><span class="op">$</span><span class="va">seLogRr</span><span class="op">)</span></span>
<span><span class="va">null</span></span></code></pre></div>
<pre><code><span><span class="co">## Estimated null distribution</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##      Estimate</span></span>
<span><span class="co">## Mean   0.7920</span></span>
<span><span class="co">## SD     0.2835</span></span></code></pre>
<p>We see that the mean of our distribution is greater than 0,
indicating the analysis is positively biased. We also see the standard
deviation is greater than 0.25, indicating there is considerable
variability in the systematic error from one estimate to the next.</p>
</div>
<div class="section level3">
<h3 id="evaluating-the-calibration">Evaluating the calibration<a class="anchor" aria-label="anchor" href="#evaluating-the-calibration"></a>
</h3>
<p>To evaluate whether our estimation of the systematic error
distribution is a good one, we can test whether the calibrated p-value
is truly calibrated, meaning the fraction of negative controls with a
p-value below alpha is approximately the same as alpha:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotCalibration.html">plotCalibration</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">$</span><span class="va">logRr</span>,<span class="va">negatives</span><span class="op">$</span><span class="va">seLogRr</span><span class="op">)</span></span></code></pre></div>
<p><img src="EmpiricalPCalibrationVignette_files/figure-html/unnamed-chunk-6-1.png" width="700"></p>
<p>This method uses a leave-one-out design: for every negative control,
the null distribution is fitted using all other negative controls, and
the calibrated p-value for that negative control is computed.</p>
<p>In the graph we see that the calibrated p-value is much closer to the
diagonal than the uncalibrated p-value.</p>
</div>
<div class="section level3">
<h3 id="plotting-the-null-distribution">Plotting the null distribution<a class="anchor" aria-label="anchor" href="#plotting-the-null-distribution"></a>
</h3>
<p>We can create a graphical representation of the null distribution,
together with the negative controls used to estimate that
distribution:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotCalibrationEffect.html">plotCalibrationEffect</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">$</span><span class="va">logRr</span>,<span class="va">negatives</span><span class="op">$</span><span class="va">seLogRr</span>, null <span class="op">=</span> <span class="va">null</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Warning: Removed 1 row containing missing values or values outside the scale range</span></span>
<span><span class="co">## (`geom_vline()`).</span></span></code></pre>
<p><img src="EmpiricalPCalibrationVignette_files/figure-html/unnamed-chunk-7-1.png" width="700"></p>
<p>In this graph, the blue dots represent the negative controls. Any
estimates below the gray dashed lines will have a traditional p-value
below .05. In contrast, only estimates that fall within the orange areas
will have a calibrated p-value below .05.</p>
</div>
</div>
<div class="section level2">
<h2 id="p-value-calibration">P-value calibration<a class="anchor" aria-label="anchor" href="#p-value-calibration"></a>
</h2>
<div class="section level3">
<h3 id="calibrating-the-p-value">Calibrating the p-value<a class="anchor" aria-label="anchor" href="#calibrating-the-p-value"></a>
</h3>
<p>We can now use the estimated null distribution to compute the
calibrated p-value for our drug of interest:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/calibrateP.html">calibrateP</a></span><span class="op">(</span><span class="va">null</span>, <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">logRr</span>, <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">seLogRr</span><span class="op">)</span></span>
<span><span class="va">p</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 0.839405</span></span></code></pre>
<p>In this case, the calibrated p-value is 0.84, meaning we have very
little confidence we can reject the null hypothesis.</p>
</div>
<div class="section level3">
<h3 id="plotting-the-null-distribution-1">Plotting the null distribution<a class="anchor" aria-label="anchor" href="#plotting-the-null-distribution-1"></a>
</h3>
<p>A visual representation of the calibration makes it clear why we are
no longer certain we can reject the null hypothesis:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotCalibrationEffect.html">plotCalibrationEffect</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">$</span><span class="va">logRr</span>,</span>
<span>                      <span class="va">negatives</span><span class="op">$</span><span class="va">seLogRr</span>, </span>
<span>                      <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">logRr</span>, </span>
<span>                      <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">seLogRr</span>, </span>
<span>                      <span class="va">null</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Warning: Removed 1 row containing missing values or values outside the scale range</span></span>
<span><span class="co">## (`geom_vline()`).</span></span></code></pre>
<p><img src="EmpiricalPCalibrationVignette_files/figure-html/unnamed-chunk-9-1.png" width="700"></p>
<p>In this plot we see that, even though the drug of interest (the
yellow diamond) has a high relative risk, it is indistinguishable from
our negative controls.</p>
</div>
</div>
<div class="section level2">
<h2 id="computing-the-credible-interval">Computing the credible interval<a class="anchor" aria-label="anchor" href="#computing-the-credible-interval"></a>
</h2>
<p>Depending on how much information we have in terms of number of
negative controls, or precision of those negative controls, we will be
more or less certain about the parameters of the null distribution and
therefore about the calibrated p-value. To estimate our uncertainty we
can compute the 95% credible interval using Markov Chain Monte Carlo
(MCMC). We can apply the <code>fitMcmcNull</code> function for this
purpose:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">null</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fitMcmcNull.html">fitMcmcNull</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">$</span><span class="va">logRr</span>, <span class="va">negatives</span><span class="op">$</span><span class="va">seLogRr</span><span class="op">)</span></span>
<span><span class="va">null</span></span></code></pre></div>
<pre><code><span><span class="co">## Estimated null distribution (using MCMC)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Estimate lower .95 upper .95</span></span>
<span><span class="co">## Mean       0.79140   0.66974    0.9053</span></span>
<span><span class="co">## Precision 11.91267   6.04624   23.2171</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Acceptance rate: 0.3259967400326</span></span></code></pre>
<p>We see that there is uncertainty around the estimates of the mean and
precision (= 1/SD^2), as expressed in the 95% credible intervals. This
uncertainty can be reduced by either increasing the number of negative
controls, or by increasing the power for the existing controls (e.g. by
waiting for more data to accumulate).</p>
<p>The acceptance rate of the MCMC seems reasonable (ideal values are
typically between 0.2 and 0.6), but we can investigate the trace just to
be sure:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotMcmcTrace.html">plotMcmcTrace</a></span><span class="op">(</span><span class="va">null</span><span class="op">)</span></span></code></pre></div>
<p><img src="EmpiricalPCalibrationVignette_files/figure-html/unnamed-chunk-11-1.png" width="700"></p>
<p>For both variables the trace should look like ‘random noise’, as is
the case above. When we see auto-correlation, meaning that one value of
the trace depends on the previous value of the trace, the MCMC might not
be reliable and we should not trust the 95% credible interval.</p>
<p>We can use the new null object to compute the calibrated p-value as
well as the 95% credible interval:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/calibrateP.html">calibrateP</a></span><span class="op">(</span><span class="va">null</span>, <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">logRr</span>, <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">seLogRr</span><span class="op">)</span></span>
<span><span class="va">p</span></span></code></pre></div>
<pre><code><span><span class="co">##           p    lb95ci  ub95ci</span></span>
<span><span class="co">## 1 0.8359202 0.5466381 0.99161</span></span></code></pre>
<p>Note that there is uncertainty around the calibrated p-value as
expressed in the 95% credible interval.</p>
<p>We can also visualize the uncertainty in the p-value calibration by
plotting the 95% credible interval of the boundary where calibrated p =
0.05, here indicated by the red band:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plotCalibrationEffect.html">plotCalibrationEffect</a></span><span class="op">(</span><span class="va">negatives</span><span class="op">$</span><span class="va">logRr</span>,</span>
<span>                      <span class="va">negatives</span><span class="op">$</span><span class="va">seLogRr</span>, </span>
<span>                      <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">logRr</span>, </span>
<span>                      <span class="va">drugOfInterest</span><span class="op">$</span><span class="va">seLogRr</span>, </span>
<span>                      <span class="va">null</span>,</span>
<span>                      showCis <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Warning: Removed 1 row containing missing values or values outside the scale range</span></span>
<span><span class="co">## (`geom_vline()`).</span></span></code></pre>
<p><img src="EmpiricalPCalibrationVignette_files/figure-html/unnamed-chunk-13-1.png" width="700"></p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/citation.html" class="external-link">citation</a></span><span class="op">(</span><span class="st">"EmpiricalCalibration"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## To cite EmpiricalCalibration in publications use:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##   Schuemie MJ, Ryan PB, DuMouchel W, Suchard MA, Madigan D (2013).</span></span>
<span><span class="co">##   "Interpreting observational studies: why empirical calibration is</span></span>
<span><span class="co">##   needed to correct p-values." _Statistics in Medicine_, *33*(2),</span></span>
<span><span class="co">##   209-218. &lt;http://dx.doi.org/10.1002/sim.5925&gt;.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##   Schuemie MJ, Hripcsak G, Ryan PB, Madigan D, Suchard MA (2018).</span></span>
<span><span class="co">##   "Empirical confidence interval calibration for population-level</span></span>
<span><span class="co">##   effect estimation studies in observational healthcare data." _Proc.</span></span>
<span><span class="co">##   Natl. Acad. Sci. U.S.A._, *115*(11), 2571-2577.</span></span>
<span><span class="co">##   &lt;https://doi.org/10.1073/pnas.1708282114&gt;.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## To see these entries in BibTeX format, use 'print(&lt;citation&gt;,</span></span>
<span><span class="co">## bibtex=TRUE)', 'toBibtex(.)', or set</span></span>
<span><span class="co">## 'options(citation.bibtex.max=999)'.</span></span></code></pre>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Martijn Schuemie, Marc Suchard.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

      </footer>
</div>






  </body>
</html>
